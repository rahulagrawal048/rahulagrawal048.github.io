<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Rahul Agrawal</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h1>Rahul Agrawal</h1>
						<p>Robotics Graduate Student at University of Michigan</p>
						<ul class="actions">
							<li><a href="#main" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Projects</a></li>
							<li><a href="https://drive.google.com/file/d/1OMo5fdZI2HLEIVXOPl5DHlstNvZeqAax/view?usp=sharing">Resume</a></li>
							<li><a href="https://scholar.google.com/citations?user=vE4JmwkAAAAJ&hl=en&authuser=1">Research</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/rahul-agrawal-205773109" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
							<li><a href="https://github.com/rahulagrawal048" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
									<h2>Hello!
										I'm Rahul Agrawal</h2>
									<p>I am a Robotics graduate student at University of Michigan - Ann arbor.
									I specialize in perception and deep learning for computer vision. I have both industrial
									and research experience in computer vision, SLAM, Robotics, and software engineering.
									Currently I work as a Research Assistant at <a href="https://fcav.engin.umich.edu">
									UM Ford Center for Autonomous vehicles</a>
									and work on Visual SLAM in adverse weather conditions. Prior to this, I worked as a software engineer
									for more than two years at <a href="https://www.fujikura.co.jp/eng/">Fujikura Ltd. Japan</a>.
									</p>
								</header>
								<a href="https://github.com/rahulagrawal048/transformers-for-vision" class="image main"><img width="520" height="480" src="images/car.jpg" alt="" /></a>
								<header>
									<h2>SLAM in Adverse Weather Conditions</h2>
								</header>
								<p>
									I am working on a new dataset for visual SLAM and benchmark state-of-the-art SLAM algorithms like ORB-SLAM3 and DSM.
									The dataset comprises of sequences in adverse weather conditions of rain, fog, and snow. Specific responsibilites are
									performing sensor calibration and fusion of different sensors including LIDAR, RGB cameras, monochrome cameras, thermal cameras, GNSS-INS, and event cameras
									mounted on a Ford Fusion car. I also research evaluation methods for visual SLAM and am developing an evaluation pipeline for the 
									dataset to be publicly available. I have developed a GUI application with Python and Qt to automate sensor calibration and make data collection easier.
								</p>
							</article>

						<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/Robotic-Arm">Robotic Arm for Manipulation</a></h2>
									</header>
									<iframe width="480" height="320"
										src="https://www.youtube.com/embed/JX__0Y76WpM">
									</iframe>
									<p>
										The algorithmic design of a 5 degree-of-freedom robotic arm for manipulation.
										The objective is to detect blocks, stacked or unstacked in a workspace using a RGB-D camera,
										and apply path planning and kinematics to a robotic arm to pick and place the blocks. 
										3D calibration was successfully performed for transformation between image and world coordinates.
										Block detection was implemented and blocks of various col- ors and two sizes were detected along 
										with their centroids and orientation. Inverse Kinematics and motion planning was implemented to 
										grab and place blocks according to the task.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/Exploration-bot" class="button">Check it out</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/Exploration-bot">Differential Drive Robot</a></h2>
									</header>
									<iframe width="480" height="320"
										src="https://www.youtube.com/embed/Ix8HPOaqvUw">
									</iframe>
									<p>
										A bot that autonomously explores a maze using SLAM and A* path planning. 
										The bot uses frontiers to explore the maze and maps the environment using 2D LIDAR.
										After the exploration is completed, it uses A* to make the path back to its home position. </p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/Exploration-bot" class="button">Check it out</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/transformers-for-vision">Vision Transformers for Semantic Segmentation
										</a></h2>
									</header>
									<a href="https://github.com/rahulagrawal048/transformers-for-vision" class="image fit"><img src="images/semantic.png" alt="" /></a>
									<p>Transformers have shown advanced performance in computer vision tasks over existing architectures such as RNNs and CNNs. 
									I'm responsible for benchmarking state-of-the-art Transformer algorithms on autonomous driving datasets and 
									implementing novel Transformer-based deep learning algorithms for object detection/tracking/prediction.
									This project implements a transformer based model, Segformer and benchmark on Cityscapes dataset for semantic segmentation.
									The model achieves a mean Intersection over Union (mIoU) of 84%.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/transformers-for-vision" class="button">Check it out</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/transformers-for-vision">Image Captioning using LSTM-RNN with Attention
										</a></h2>
									</header>
									<a href="https://github.com/rahulagrawal048/transformers-for-vision" class="image fit"><img src="images/image_captioning.png" alt="" /></a>
									<p>Implemented vanilla recurrent neural networks (RNNs), long-short term memory networks (LSTMs), and attention-based 
										LSTMs to train a model that can generate natural language captions for images.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/transformers-for-vision" class="button">Check it out</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/Autonomous-Courier-Robot">Autonomous Courier Robot
										</a></h2>
									</header>
									<a href="https://github.com/rahulagrawal048/Autonomous-Courier-Robot" class="image fit"><img src="images/courier.jpg" alt="" /></a>
									<p>A robot that delivers couriers is an active area of research and increasingly gaining popularity.
									This is a prototype of an autonomous bot that uses OpenCV to detect pickup and drop locations by matching the shapes and Dijksta's algorithm for shortest path planning.
									The robot takes input the top view of the arena and starts at the green node. It travels by following the black line using a camera
									and image processing, and the blue areas on the track are recognized as intersections.
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/Autonomous-Courier-Robot" class="button">Check it out</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2><a href="https://github.com/rahulagrawal048/Electricity-Price-forecasting">Electricity Price Forecasting
										</h2>
									</header>
									<a href="https://github.com/rahulagrawal048/Electricity-Price-forecasting" class="image fit"><img src="images/price.jpg" alt="" /></a>
									<p>Real-time prediction of electricity pricing signals is essential for scheduling load demand in price-directed grids.
									This project introduces a novel model for electricity locational marginal price forecasting primarily centered on Relevance Vector Machine.
									The proposed model is found to outperform the current state-of-the-art models with a Mean Absolute Error of 2.6 on the test set and 
									is sufficiently cheap computationally with a training time of 88 seconds. The work was published in 
									<a href="https://doi.org/10.1016/j.apenergy.2019.05.062">Applied Energy, Elsevier.</a>
									</p>
									<ul class="actions special">
										<li><a href="https://github.com/rahulagrawal048/Electricity-Price-forecasting" class="button">Check it out</a></li>
									</ul>
								</article>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section class="split contact">
							<section class="alt">
								<h3>Email</h3>
								<p><a href="#">rahulagr@umich.edu</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/rahul-agrawal-205773109/" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/rahulagrawal048" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>